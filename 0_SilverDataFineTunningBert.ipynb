{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "957b820d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fine-tuning a multiclass text classification task using the DistilBERT-base-uncased model from the Transformers library and saving it i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00ca50e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6ca51f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from transformers import DistilBertTokenizerFast, TFDistilBertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2125a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../0Round2TrainingSI/SI_1298LDataset.csv\")\n",
    "#replace the dataset path by your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60e6bdad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post</th>\n",
       "      <th>si</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>’ self harm year ’ look tempt ’ fuck alone fee...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>anybody suggest overdose work someone try work...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blood run vein every single cell body even bra...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>isnt first time ive felt like never felt final...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ive beenwaiting moment year everyone asleeo im...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                post  si\n",
       "0  ’ self harm year ’ look tempt ’ fuck alone fee...   1\n",
       "1  anybody suggest overdose work someone try work...   1\n",
       "2  blood run vein every single cell body even bra...   1\n",
       "3  isnt first time ive felt like never felt final...   1\n",
       "4  ive beenwaiting moment year everyone asleeo im...   1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ea2a747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1298, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c518633b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_silver2 = pd.read_csv(\"Round3_LabeledData2020_Head600.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50572232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>author</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>selftext</th>\n",
       "      <th>suicideIdeation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>trash_account173638</td>\n",
       "      <td>2020-03-01 00:02:43</td>\n",
       "      <td>recent diagnos sever ocd mind constant assault...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17</td>\n",
       "      <td>Bad_Operator</td>\n",
       "      <td>2020-03-01 00:07:16</td>\n",
       "      <td>wrote note decid slit wrist sit tub sorri son</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Bbrave1</td>\n",
       "      <td>2020-03-01 00:16:12</td>\n",
       "      <td>slept poor last night awok terribl exhaust moo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>kirin113</td>\n",
       "      <td>2020-03-01 00:23:19</td>\n",
       "      <td>hi new reddit sure right place post wonder any...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>87</td>\n",
       "      <td>stonedfordepression</td>\n",
       "      <td>2020-03-01 00:35:41</td>\n",
       "      <td>struggl mental health issu sinc kid attempt su...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0               author          created_utc  \\\n",
       "0           6  trash_account173638  2020-03-01 00:02:43   \n",
       "1          17         Bad_Operator  2020-03-01 00:07:16   \n",
       "2          38              Bbrave1  2020-03-01 00:16:12   \n",
       "3          56             kirin113  2020-03-01 00:23:19   \n",
       "4          87  stonedfordepression  2020-03-01 00:35:41   \n",
       "\n",
       "                                            selftext  suicideIdeation  \n",
       "0  recent diagnos sever ocd mind constant assault...                1  \n",
       "1      wrote note decid slit wrist sit tub sorri son                0  \n",
       "2  slept poor last night awok terribl exhaust moo...                1  \n",
       "3  hi new reddit sure right place post wonder any...                1  \n",
       "4  struggl mental health issu sinc kid attempt su...                1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_silver2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc64a1a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(538, 5)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_silver2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f5f521f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(538, 2)\n"
     ]
    }
   ],
   "source": [
    "# Select specific columns from df_silver and rename them\n",
    "df_silver2 = df_silver2[['selftext', 'suicideIdeation']]\n",
    "df_silver2.columns = ['post', 'si']\n",
    "print(df_silver2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55ffb904",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1298, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Select specific columns from df\n",
    "df = df[['post', 'si']]\n",
    "df.columns = ['post', 'si']\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b70aabff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(538, 2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_silver2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2fd4dad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the dataframes\n",
    "df_1800 = pd.concat([df_silver2, df], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8bfa67e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1836, 2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1800.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b1853f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the new dataframe to CSV\n",
    "df_1800.to_csv(\"SI_Round3_1836_LDataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37cb27ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"SI_Round3_1836_LDataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32fd6e12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1836, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c10d557a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post</th>\n",
       "      <th>si</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>recent diagnos sever ocd mind constant assault...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wrote note decid slit wrist sit tub sorri son</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>slept poor last night awok terribl exhaust moo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hi new reddit sure right place post wonder any...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>struggl mental health issu sinc kid attempt su...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>everyth life slowli gotten wors wors use go un...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>serious contempl suicid enjoy life want contin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>go ignor peopl tell know research pleas let know</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pleas know painless relat simpl way commit sui...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>went first therapi session last tuesday resche...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                post  si\n",
       "0  recent diagnos sever ocd mind constant assault...   1\n",
       "1      wrote note decid slit wrist sit tub sorri son   0\n",
       "2  slept poor last night awok terribl exhaust moo...   1\n",
       "3  hi new reddit sure right place post wonder any...   1\n",
       "4  struggl mental health issu sinc kid attempt su...   1\n",
       "5  everyth life slowli gotten wors wors use go un...   0\n",
       "6  serious contempl suicid enjoy life want contin...   1\n",
       "7   go ignor peopl tell know research pleas let know   0\n",
       "8  pleas know painless relat simpl way commit sui...   1\n",
       "9  went first therapi session last tuesday resche...   0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5a6c529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of posts with a suicide label of 1: 1035\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming you have the dataset stored in a pandas DataFrame called 'data'\n",
    "# You can modify the code accordingly based on your specific dataset structure\n",
    "\n",
    "suicide_count = 0\n",
    "for index, row in df.iterrows():\n",
    "    if row['si'] == 1:\n",
    "        suicide_count += 1\n",
    "\n",
    "print(\"Number of posts with a suicide label of 1:\", suicide_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "511fa557",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import contractions\n",
    "def clean_text(text):\n",
    "    # Replace missing values with empty strings\n",
    "    text = str(text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    #fix the contraction \n",
    "    text = contractions.fix(text)\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "\n",
    "    # Remove mentions and hashtags\n",
    "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Tokenize text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    # Lowercase text\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "\n",
    "    #Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Stem words\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    # Join tokens into a string\n",
    "    text = ' '.join(tokens)\n",
    "    \n",
    "    return(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "238e90c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/jzhu10/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59a9b445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the 'post' column\n",
    "df['cleaned_post'] = df['post'].apply(lambda x: clean_text(x) if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a62627ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post</th>\n",
       "      <th>si</th>\n",
       "      <th>cleaned_post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1833</th>\n",
       "      <td>['That sounds really weird.Maybe you were Dist...</td>\n",
       "      <td>0</td>\n",
       "      <td>sound realli weirdmayb distract someth sound h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1834</th>\n",
       "      <td>['Dont know there as dumb as it sounds I feel ...</td>\n",
       "      <td>1</td>\n",
       "      <td>know dumb sound feel hyperact behavior even de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1835</th>\n",
       "      <td>['&amp;gt;It gets better, trust me.Ive spent long ...</td>\n",
       "      <td>0</td>\n",
       "      <td>gtit get better trust mei spent long enough bl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   post  si  \\\n",
       "1833  ['That sounds really weird.Maybe you were Dist...   0   \n",
       "1834  ['Dont know there as dumb as it sounds I feel ...   1   \n",
       "1835  ['&gt;It gets better, trust me.Ive spent long ...   0   \n",
       "\n",
       "                                           cleaned_post  \n",
       "1833  sound realli weirdmayb distract someth sound h...  \n",
       "1834  know dumb sound feel hyperact behavior even de...  \n",
       "1835  gtit get better trust mei spent long enough bl...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ff8244b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and validation sets\n",
    "# Split the data into train and test sets\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(df[\"cleaned_post\"], df[\"si\"], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8e883ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer and encode the texts\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "train_encodings = tokenizer(train_texts.tolist(), truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_texts.tolist(), truncation=True, padding=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "096ba83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data to tf.data.Dataset format\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), train_labels))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), test_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "12321020",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_projector', 'activation_13', 'vocab_transform', 'vocab_layer_norm']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier', 'classifier', 'dropout_19']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jzhu10/.conda/envs/twitter/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py:376: FutureWarning: The old compute_loss method is deprecated as it conflicts with the Keras compute_loss method added in TF 2.8. If you want the original HF compute_loss, please call hf_compute_loss() instead. From TF versions >= 2.8, or Transformers versions >= 5, calling compute_loss() will get the Keras method instead.\n",
      "  return py_builtins.overload_of(f)(*args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92/92 [==============================] - 369s 4s/step - loss: 0.6803 - accuracy: 0.5702 - val_loss: 0.6297 - val_accuracy: 0.6467\n",
      "Epoch 2/10\n",
      "92/92 [==============================] - 362s 4s/step - loss: 0.6098 - accuracy: 0.6662 - val_loss: 0.5942 - val_accuracy: 0.7038\n",
      "Epoch 3/10\n",
      "92/92 [==============================] - 353s 4s/step - loss: 0.4822 - accuracy: 0.7643 - val_loss: 0.5555 - val_accuracy: 0.7283\n",
      "Epoch 4/10\n",
      "92/92 [==============================] - 351s 4s/step - loss: 0.3397 - accuracy: 0.8549 - val_loss: 0.7166 - val_accuracy: 0.6957\n",
      "Epoch 5/10\n",
      "92/92 [==============================] - 351s 4s/step - loss: 0.1918 - accuracy: 0.9203 - val_loss: 0.8412 - val_accuracy: 0.7011\n",
      "Epoch 6/10\n",
      "92/92 [==============================] - 352s 4s/step - loss: 0.1148 - accuracy: 0.9557 - val_loss: 0.9383 - val_accuracy: 0.6875\n",
      "Epoch 7/10\n",
      "92/92 [==============================] - 352s 4s/step - loss: 0.0833 - accuracy: 0.9714 - val_loss: 1.0958 - val_accuracy: 0.6739\n",
      "Epoch 8/10\n",
      "92/92 [==============================] - 350s 4s/step - loss: 0.0689 - accuracy: 0.9721 - val_loss: 1.1786 - val_accuracy: 0.6902\n",
      "Epoch 9/10\n",
      "92/92 [==============================] - 352s 4s/step - loss: 0.0448 - accuracy: 0.9823 - val_loss: 1.4253 - val_accuracy: 0.6984\n",
      "Epoch 10/10\n",
      "92/92 [==============================] - 353s 4s/step - loss: 0.0213 - accuracy: 0.9939 - val_loss: 1.5568 - val_accuracy: 0.7174\n",
      "Accuracy: 0.717391304347826\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./1300Model1/tokenizer_config.json',\n",
       " './1300Model1/special_tokens_map.json',\n",
       " './1300Model1/vocab.txt',\n",
       " './1300Model1/added_tokens.json',\n",
       " './1300Model1/tokenizer.json')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the model\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=4e-5),\n",
    "              loss=model.compute_loss,\n",
    "              metrics=['accuracy'])\n",
    "# Train the model\n",
    "model.fit(train_dataset.shuffle(len(train_dataset)).batch(16),\n",
    "          epochs=10,\n",
    "          batch_size=16,\n",
    "          validation_data=test_dataset.shuffle(len(test_dataset)).batch(16))\n",
    "import numpy as np\n",
    "# Evaluate the model on test data\n",
    "preds = model.predict(test_dataset.batch(16))\n",
    "pred_labels = np.argmax(preds.logits, axis=1)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = np.mean(pred_labels == test_labels)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained('./1300Model1')\n",
    "tokenizer.save_pretrained('./1300Model1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "36805ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_projector', 'activation_13', 'vocab_transform', 'vocab_layer_norm']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier', 'classifier', 'dropout_39']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jzhu10/.conda/envs/twitter/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py:376: FutureWarning: The old compute_loss method is deprecated as it conflicts with the Keras compute_loss method added in TF 2.8. If you want the original HF compute_loss, please call hf_compute_loss() instead. From TF versions >= 2.8, or Transformers versions >= 5, calling compute_loss() will get the Keras method instead.\n",
      "  return py_builtins.overload_of(f)(*args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92/92 [==============================] - 357s 4s/step - loss: 0.6806 - accuracy: 0.5817 - val_loss: 0.6441 - val_accuracy: 0.6495\n",
      "Epoch 2/10\n",
      "92/92 [==============================] - 352s 4s/step - loss: 0.6236 - accuracy: 0.6594 - val_loss: 0.5510 - val_accuracy: 0.6929\n",
      "Epoch 3/10\n",
      "92/92 [==============================] - 354s 4s/step - loss: 0.4975 - accuracy: 0.7568 - val_loss: 0.5403 - val_accuracy: 0.7092\n",
      "Epoch 4/10\n",
      "92/92 [==============================] - 355s 4s/step - loss: 0.3608 - accuracy: 0.8399 - val_loss: 0.8004 - val_accuracy: 0.6332\n",
      "Epoch 5/10\n",
      "92/92 [==============================] - 355s 4s/step - loss: 0.2404 - accuracy: 0.8971 - val_loss: 0.7360 - val_accuracy: 0.6821\n",
      "Epoch 6/10\n",
      "92/92 [==============================] - 351s 4s/step - loss: 0.1244 - accuracy: 0.9659 - val_loss: 1.0613 - val_accuracy: 0.6821\n",
      "Epoch 7/10\n",
      "92/92 [==============================] - 349s 4s/step - loss: 0.0740 - accuracy: 0.9734 - val_loss: 1.0451 - val_accuracy: 0.7092\n",
      "Epoch 8/10\n",
      "92/92 [==============================] - 348s 4s/step - loss: 0.0583 - accuracy: 0.9823 - val_loss: 1.1783 - val_accuracy: 0.6902\n",
      "Epoch 9/10\n",
      "92/92 [==============================] - 349s 4s/step - loss: 0.0296 - accuracy: 0.9925 - val_loss: 1.5355 - val_accuracy: 0.6793\n",
      "Epoch 10/10\n",
      "92/92 [==============================] - 351s 4s/step - loss: 0.0119 - accuracy: 0.9966 - val_loss: 1.7433 - val_accuracy: 0.7065\n",
      "Accuracy: 0.7065217391304348\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./1800Model2/tokenizer_config.json',\n",
       " './1800Model2/special_tokens_map.json',\n",
       " './1800Model2/vocab.txt',\n",
       " './1800Model2/added_tokens.json',\n",
       " './1800Model2/tokenizer.json')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the model\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
    "              loss=model.compute_loss,\n",
    "              metrics=['accuracy'])\n",
    "# Train the model\n",
    "model.fit(train_dataset.shuffle(len(train_dataset)).batch(16),\n",
    "          epochs=10,\n",
    "          batch_size=32,\n",
    "          validation_data=test_dataset.shuffle(len(test_dataset)).batch(16))\n",
    "import numpy as np\n",
    "# Evaluate the model on test data\n",
    "preds = model.predict(test_dataset.batch(16))\n",
    "pred_labels = np.argmax(preds.logits, axis=1)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = np.mean(pred_labels == test_labels)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained('./1800Model2')\n",
    "tokenizer.save_pretrained('./1800Model2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7b858279",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_projector', 'activation_13', 'vocab_transform', 'vocab_layer_norm']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier', 'pre_classifier', 'dropout_59']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jzhu10/.conda/envs/twitter/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py:376: FutureWarning: The old compute_loss method is deprecated as it conflicts with the Keras compute_loss method added in TF 2.8. If you want the original HF compute_loss, please call hf_compute_loss() instead. From TF versions >= 2.8, or Transformers versions >= 5, calling compute_loss() will get the Keras method instead.\n",
      "  return py_builtins.overload_of(f)(*args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92/92 [==============================] - 359s 4s/step - loss: 0.6787 - accuracy: 0.5688 - val_loss: 0.6333 - val_accuracy: 0.6386\n",
      "Epoch 2/20\n",
      "92/92 [==============================] - 353s 4s/step - loss: 0.6096 - accuracy: 0.6805 - val_loss: 0.5446 - val_accuracy: 0.7391\n",
      "Epoch 3/20\n",
      "92/92 [==============================] - 345s 4s/step - loss: 0.4965 - accuracy: 0.7568 - val_loss: 0.5639 - val_accuracy: 0.6957\n",
      "Epoch 4/20\n",
      "92/92 [==============================] - 351s 4s/step - loss: 0.3826 - accuracy: 0.8283 - val_loss: 0.6474 - val_accuracy: 0.6902\n",
      "Epoch 5/20\n",
      "92/92 [==============================] - 356s 4s/step - loss: 0.2354 - accuracy: 0.8992 - val_loss: 0.8070 - val_accuracy: 0.6848\n",
      "Epoch 6/20\n",
      "92/92 [==============================] - 353s 4s/step - loss: 0.1355 - accuracy: 0.9503 - val_loss: 1.0562 - val_accuracy: 0.7011\n",
      "Epoch 7/20\n",
      "92/92 [==============================] - 350s 4s/step - loss: 0.1156 - accuracy: 0.9591 - val_loss: 1.1792 - val_accuracy: 0.6984\n",
      "Epoch 8/20\n",
      "92/92 [==============================] - 349s 4s/step - loss: 0.0425 - accuracy: 0.9857 - val_loss: 1.2104 - val_accuracy: 0.6902\n",
      "Epoch 9/20\n",
      "92/92 [==============================] - 350s 4s/step - loss: 0.0572 - accuracy: 0.9809 - val_loss: 1.1000 - val_accuracy: 0.6793\n",
      "Epoch 10/20\n",
      "92/92 [==============================] - 349s 4s/step - loss: 0.0573 - accuracy: 0.9823 - val_loss: 1.2125 - val_accuracy: 0.7201\n",
      "Epoch 11/20\n",
      "92/92 [==============================] - 350s 4s/step - loss: 0.0360 - accuracy: 0.9850 - val_loss: 1.4731 - val_accuracy: 0.6984\n",
      "Epoch 12/20\n",
      "92/92 [==============================] - 350s 4s/step - loss: 0.0220 - accuracy: 0.9939 - val_loss: 1.6219 - val_accuracy: 0.6875\n",
      "Epoch 13/20\n",
      "92/92 [==============================] - 349s 4s/step - loss: 0.0089 - accuracy: 0.9980 - val_loss: 1.6702 - val_accuracy: 0.6793\n",
      "Epoch 14/20\n",
      "92/92 [==============================] - 350s 4s/step - loss: 0.0251 - accuracy: 0.9918 - val_loss: 1.4891 - val_accuracy: 0.6875\n",
      "Epoch 15/20\n",
      "92/92 [==============================] - 353s 4s/step - loss: 0.0216 - accuracy: 0.9932 - val_loss: 1.5895 - val_accuracy: 0.6902\n",
      "Epoch 16/20\n",
      "92/92 [==============================] - 353s 4s/step - loss: 0.0150 - accuracy: 0.9952 - val_loss: 1.6556 - val_accuracy: 0.6821\n",
      "Epoch 17/20\n",
      "92/92 [==============================] - 352s 4s/step - loss: 0.0060 - accuracy: 0.9986 - val_loss: 1.7548 - val_accuracy: 0.7065\n",
      "Epoch 18/20\n",
      "92/92 [==============================] - 353s 4s/step - loss: 0.0342 - accuracy: 0.9905 - val_loss: 1.3150 - val_accuracy: 0.6522\n",
      "Epoch 19/20\n",
      "92/92 [==============================] - 351s 4s/step - loss: 0.0184 - accuracy: 0.9952 - val_loss: 1.6463 - val_accuracy: 0.6984\n",
      "Epoch 20/20\n",
      "92/92 [==============================] - 350s 4s/step - loss: 0.0076 - accuracy: 0.9973 - val_loss: 1.9171 - val_accuracy: 0.6386\n",
      "23/23 [==============================] - 28s 1s/step - loss: 1.9171 - accuracy: 0.6386\n",
      "Loss: 1.9171167612075806, Accuracy: 0.6385869383811951\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./1800Model3/tokenizer_config.json',\n",
       " './1800Model3/special_tokens_map.json',\n",
       " './1800Model3/vocab.txt',\n",
       " './1800Model3/added_tokens.json',\n",
       " './1800Model3/tokenizer.json')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the model\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5),\n",
    "              loss=model.compute_loss,\n",
    "              metrics=['accuracy'])\n",
    "# Train the model\n",
    "model.fit(train_dataset.shuffle(len(train_dataset)).batch(16),\n",
    "          epochs=20,\n",
    "          batch_size=32,\n",
    "          validation_data=test_dataset.shuffle(len(test_dataset)).batch(16))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(test_dataset.batch(16))\n",
    "\n",
    "print(f\"Loss: {loss}, Accuracy: {accuracy}\")\n",
    "# Save the model\n",
    "model.save_pretrained('./1800Model3')\n",
    "tokenizer.save_pretrained('./1800Model3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a1202a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_projector', 'activation_13', 'vocab_transform', 'vocab_layer_norm']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dropout_79', 'pre_classifier', 'classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "92/92 [==============================] - 358s 4s/step - loss: 0.6733 - accuracy: 0.5920 - val_loss: 0.6554 - val_accuracy: 0.6630\n",
      "Epoch 2/30\n",
      "92/92 [==============================] - 351s 4s/step - loss: 0.6350 - accuracy: 0.6546 - val_loss: 0.6485 - val_accuracy: 0.6223\n",
      "Epoch 3/30\n",
      "92/92 [==============================] - 350s 4s/step - loss: 0.5436 - accuracy: 0.7316 - val_loss: 0.6161 - val_accuracy: 0.7011\n",
      "Epoch 4/30\n",
      "92/92 [==============================] - 353s 4s/step - loss: 0.4418 - accuracy: 0.8052 - val_loss: 0.6319 - val_accuracy: 0.6658\n",
      "Epoch 5/30\n",
      "92/92 [==============================] - 352s 4s/step - loss: 0.3238 - accuracy: 0.8699 - val_loss: 0.6385 - val_accuracy: 0.7011\n",
      "Epoch 6/30\n",
      "92/92 [==============================] - 349s 4s/step - loss: 0.2114 - accuracy: 0.9285 - val_loss: 0.8045 - val_accuracy: 0.6929\n",
      "Epoch 7/30\n",
      "92/92 [==============================] - 351s 4s/step - loss: 0.1331 - accuracy: 0.9544 - val_loss: 0.9431 - val_accuracy: 0.6929\n",
      "Epoch 8/30\n",
      "92/92 [==============================] - 350s 4s/step - loss: 0.0948 - accuracy: 0.9687 - val_loss: 0.9287 - val_accuracy: 0.7174\n",
      "Epoch 9/30\n",
      "92/92 [==============================] - 351s 4s/step - loss: 0.0587 - accuracy: 0.9823 - val_loss: 1.1764 - val_accuracy: 0.6739\n",
      "Epoch 10/30\n",
      "92/92 [==============================] - 352s 4s/step - loss: 0.0430 - accuracy: 0.9871 - val_loss: 1.2563 - val_accuracy: 0.6875\n",
      "Epoch 11/30\n",
      "92/92 [==============================] - 352s 4s/step - loss: 0.0707 - accuracy: 0.9789 - val_loss: 1.1001 - val_accuracy: 0.6875\n",
      "Epoch 12/30\n",
      "92/92 [==============================] - 351s 4s/step - loss: 0.0431 - accuracy: 0.9884 - val_loss: 1.2405 - val_accuracy: 0.7011\n",
      "Epoch 13/30\n",
      "92/92 [==============================] - 350s 4s/step - loss: 0.0347 - accuracy: 0.9871 - val_loss: 1.3546 - val_accuracy: 0.6848\n",
      "Epoch 14/30\n",
      "92/92 [==============================] - 352s 4s/step - loss: 0.0150 - accuracy: 0.9973 - val_loss: 1.3133 - val_accuracy: 0.7174\n",
      "Epoch 15/30\n",
      "92/92 [==============================] - 352s 4s/step - loss: 0.0091 - accuracy: 0.9980 - val_loss: 1.2968 - val_accuracy: 0.7147\n",
      "Epoch 16/30\n",
      "92/92 [==============================] - 353s 4s/step - loss: 0.0069 - accuracy: 0.9986 - val_loss: 1.3568 - val_accuracy: 0.7038\n",
      "Epoch 17/30\n",
      "92/92 [==============================] - 353s 4s/step - loss: 0.0142 - accuracy: 0.9952 - val_loss: 1.4847 - val_accuracy: 0.6739\n",
      "Epoch 18/30\n",
      "92/92 [==============================] - 355s 4s/step - loss: 0.0301 - accuracy: 0.9918 - val_loss: 1.3469 - val_accuracy: 0.6957\n",
      "Epoch 19/30\n",
      "92/92 [==============================] - 353s 4s/step - loss: 0.0100 - accuracy: 0.9966 - val_loss: 1.4104 - val_accuracy: 0.7038\n",
      "Epoch 20/30\n",
      "92/92 [==============================] - 354s 4s/step - loss: 0.0219 - accuracy: 0.9925 - val_loss: 1.6124 - val_accuracy: 0.6902\n",
      "Epoch 21/30\n",
      "92/92 [==============================] - 352s 4s/step - loss: 0.0386 - accuracy: 0.9871 - val_loss: 1.5095 - val_accuracy: 0.6875\n",
      "Epoch 22/30\n",
      "92/92 [==============================] - 353s 4s/step - loss: 0.0095 - accuracy: 0.9986 - val_loss: 1.5678 - val_accuracy: 0.7011\n",
      "Epoch 23/30\n",
      "92/92 [==============================] - 352s 4s/step - loss: 0.0222 - accuracy: 0.9939 - val_loss: 1.5198 - val_accuracy: 0.6793\n",
      "Epoch 24/30\n",
      "92/92 [==============================] - 352s 4s/step - loss: 0.0059 - accuracy: 0.9973 - val_loss: 1.5215 - val_accuracy: 0.6766\n",
      "Epoch 25/30\n",
      "92/92 [==============================] - 355s 4s/step - loss: 0.0105 - accuracy: 0.9980 - val_loss: 1.6040 - val_accuracy: 0.6793\n",
      "Epoch 26/30\n",
      "92/92 [==============================] - 357s 4s/step - loss: 0.0176 - accuracy: 0.9918 - val_loss: 1.6090 - val_accuracy: 0.6902\n",
      "Epoch 27/30\n",
      "92/92 [==============================] - 355s 4s/step - loss: 0.0091 - accuracy: 0.9980 - val_loss: 1.6304 - val_accuracy: 0.6821\n",
      "Epoch 28/30\n",
      "92/92 [==============================] - 352s 4s/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 1.6727 - val_accuracy: 0.6875\n",
      "Epoch 29/30\n",
      "92/92 [==============================] - 350s 4s/step - loss: 0.0375 - accuracy: 0.9871 - val_loss: 1.7022 - val_accuracy: 0.6848\n",
      "Epoch 30/30\n",
      "92/92 [==============================] - 352s 4s/step - loss: 0.0175 - accuracy: 0.9959 - val_loss: 1.6898 - val_accuracy: 0.6902\n",
      "23/23 [==============================] - 28s 1s/step - loss: 1.6898 - accuracy: 0.6902\n",
      "Loss: 1.6898311376571655, Accuracy: 0.6902173757553101\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./1800Model1_7/tokenizer_config.json',\n",
       " './1800Model1_7/special_tokens_map.json',\n",
       " './1800Model1_7/vocab.txt',\n",
       " './1800Model1_7/added_tokens.json',\n",
       " './1800Model1_7/tokenizer.json')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the model\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "# Compile the model\n",
    "\n",
    "# Compile the model with adjusted learning rate\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
    "              loss=model.compute_loss,\n",
    "              metrics=['accuracy'])\n",
    "# Train the model with increased epochs\n",
    "model.fit(train_dataset.shuffle(len(train_dataset)).batch(16),\n",
    "          epochs=30,\n",
    "          batch_size=32,\n",
    "          validation_data=test_dataset.shuffle(len(test_dataset)).batch(16))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(test_dataset.batch(16))\n",
    "\n",
    "print(f\"Loss: {loss}, Accuracy: {accuracy}\")\n",
    "# Save the model\n",
    "model.save_pretrained('./1800Model1_7')\n",
    "tokenizer.save_pretrained('./1800Model1_7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6835cdc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['activation_13', 'vocab_transform', 'vocab_projector', 'vocab_layer_norm']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier', 'pre_classifier', 'dropout_63']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "46/46 [==============================] - 411s 9s/step - loss: 0.6802 - accuracy: 0.5579 - val_loss: 0.6639 - val_accuracy: 0.5625\n",
      "Epoch 2/50\n",
      "46/46 [==============================] - 413s 9s/step - loss: 0.6189 - accuracy: 0.6553 - val_loss: 0.5832 - val_accuracy: 0.6766\n",
      "Epoch 3/50\n",
      "46/46 [==============================] - 406s 9s/step - loss: 0.5215 - accuracy: 0.7302 - val_loss: 0.5656 - val_accuracy: 0.7065\n",
      "Epoch 4/50\n",
      "46/46 [==============================] - 404s 9s/step - loss: 0.3818 - accuracy: 0.8324 - val_loss: 0.6486 - val_accuracy: 0.6957\n",
      "Epoch 5/50\n",
      "46/46 [==============================] - 401s 9s/step - loss: 0.2216 - accuracy: 0.9128 - val_loss: 0.7474 - val_accuracy: 0.7092\n",
      "Epoch 6/50\n",
      "46/46 [==============================] - 401s 9s/step - loss: 0.1364 - accuracy: 0.9455 - val_loss: 1.0343 - val_accuracy: 0.6848\n",
      "Epoch 7/50\n",
      "46/46 [==============================] - 399s 9s/step - loss: 0.1375 - accuracy: 0.9462 - val_loss: 0.9602 - val_accuracy: 0.7011\n",
      "Epoch 8/50\n",
      "46/46 [==============================] - 398s 9s/step - loss: 0.0576 - accuracy: 0.9816 - val_loss: 1.2556 - val_accuracy: 0.6685\n",
      "Epoch 9/50\n",
      "46/46 [==============================] - 398s 9s/step - loss: 0.0465 - accuracy: 0.9802 - val_loss: 1.2726 - val_accuracy: 0.6630\n",
      "Epoch 10/50\n",
      "46/46 [==============================] - 405s 9s/step - loss: 0.0298 - accuracy: 0.9877 - val_loss: 1.3938 - val_accuracy: 0.7255\n",
      "Epoch 11/50\n",
      "46/46 [==============================] - 401s 9s/step - loss: 0.0331 - accuracy: 0.9925 - val_loss: 1.4009 - val_accuracy: 0.7011\n",
      "Epoch 12/50\n",
      "46/46 [==============================] - 400s 9s/step - loss: 0.0364 - accuracy: 0.9918 - val_loss: 1.3905 - val_accuracy: 0.6793\n",
      "Epoch 13/50\n",
      "46/46 [==============================] - 401s 9s/step - loss: 0.0196 - accuracy: 0.9925 - val_loss: 1.4672 - val_accuracy: 0.6875\n",
      "Epoch 14/50\n",
      "46/46 [==============================] - 401s 9s/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 1.6697 - val_accuracy: 0.6957\n",
      "Epoch 15/50\n",
      "46/46 [==============================] - 403s 9s/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 1.7421 - val_accuracy: 0.6957\n",
      "Epoch 16/50\n",
      "46/46 [==============================] - 399s 9s/step - loss: 8.9676e-04 - accuracy: 1.0000 - val_loss: 1.8209 - val_accuracy: 0.6957\n",
      "Epoch 17/50\n",
      "46/46 [==============================] - 400s 9s/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 1.8945 - val_accuracy: 0.6984\n",
      "Epoch 18/50\n",
      "46/46 [==============================] - 399s 9s/step - loss: 0.0149 - accuracy: 0.9959 - val_loss: 1.6758 - val_accuracy: 0.7174\n",
      "Epoch 19/50\n",
      "46/46 [==============================] - 397s 9s/step - loss: 0.0231 - accuracy: 0.9932 - val_loss: 1.6561 - val_accuracy: 0.6957\n",
      "Epoch 20/50\n",
      "46/46 [==============================] - 401s 9s/step - loss: 0.0149 - accuracy: 0.9966 - val_loss: 1.5998 - val_accuracy: 0.6848\n",
      "Epoch 21/50\n",
      "46/46 [==============================] - 400s 9s/step - loss: 0.0076 - accuracy: 0.9980 - val_loss: 1.7288 - val_accuracy: 0.6739\n",
      "Epoch 22/50\n",
      "46/46 [==============================] - 398s 9s/step - loss: 0.0286 - accuracy: 0.9891 - val_loss: 1.4465 - val_accuracy: 0.7174\n",
      "Epoch 23/50\n",
      "46/46 [==============================] - 398s 9s/step - loss: 0.0270 - accuracy: 0.9925 - val_loss: 1.5755 - val_accuracy: 0.6929\n",
      "Epoch 24/50\n",
      "46/46 [==============================] - 396s 9s/step - loss: 0.0314 - accuracy: 0.9905 - val_loss: 1.5278 - val_accuracy: 0.7228\n",
      "Epoch 25/50\n",
      "46/46 [==============================] - 396s 9s/step - loss: 0.0214 - accuracy: 0.9925 - val_loss: 1.6412 - val_accuracy: 0.6712\n",
      "Epoch 26/50\n",
      "46/46 [==============================] - 400s 9s/step - loss: 0.0185 - accuracy: 0.9939 - val_loss: 1.7153 - val_accuracy: 0.6929\n",
      "Epoch 27/50\n",
      "46/46 [==============================] - 396s 9s/step - loss: 0.0202 - accuracy: 0.9911 - val_loss: 1.6733 - val_accuracy: 0.6929\n",
      "Epoch 28/50\n",
      "46/46 [==============================] - 396s 9s/step - loss: 0.0141 - accuracy: 0.9932 - val_loss: 1.7185 - val_accuracy: 0.6821\n",
      "Epoch 29/50\n",
      "46/46 [==============================] - 395s 9s/step - loss: 0.0122 - accuracy: 0.9986 - val_loss: 1.7507 - val_accuracy: 0.6821\n",
      "Epoch 30/50\n",
      "46/46 [==============================] - 395s 9s/step - loss: 0.0078 - accuracy: 0.9973 - val_loss: 1.7584 - val_accuracy: 0.6984\n",
      "Epoch 31/50\n",
      "46/46 [==============================] - 395s 9s/step - loss: 0.0028 - accuracy: 0.9993 - val_loss: 2.3623 - val_accuracy: 0.6332\n",
      "Epoch 32/50\n",
      "46/46 [==============================] - 394s 9s/step - loss: 0.0332 - accuracy: 0.9925 - val_loss: 1.6272 - val_accuracy: 0.6793\n",
      "Epoch 33/50\n",
      "46/46 [==============================] - 391s 9s/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 1.8390 - val_accuracy: 0.6766\n",
      "Epoch 34/50\n",
      "46/46 [==============================] - 390s 8s/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 2.0274 - val_accuracy: 0.6821\n",
      "Epoch 35/50\n",
      "46/46 [==============================] - 397s 9s/step - loss: 0.0012 - accuracy: 0.9993 - val_loss: 2.0545 - val_accuracy: 0.6875\n",
      "Epoch 36/50\n",
      "46/46 [==============================] - 398s 9s/step - loss: 0.0060 - accuracy: 0.9993 - val_loss: 2.0781 - val_accuracy: 0.6685\n",
      "Epoch 37/50\n",
      "46/46 [==============================] - 398s 9s/step - loss: 0.0077 - accuracy: 0.9973 - val_loss: 1.8891 - val_accuracy: 0.7174\n",
      "Epoch 38/50\n",
      "46/46 [==============================] - 398s 9s/step - loss: 0.0145 - accuracy: 0.9952 - val_loss: 1.6783 - val_accuracy: 0.7011\n",
      "Epoch 39/50\n",
      "46/46 [==============================] - 399s 9s/step - loss: 0.0188 - accuracy: 0.9939 - val_loss: 1.5167 - val_accuracy: 0.6984\n",
      "Epoch 40/50\n",
      "46/46 [==============================] - 397s 9s/step - loss: 0.0032 - accuracy: 0.9993 - val_loss: 1.6825 - val_accuracy: 0.7065\n",
      "Epoch 41/50\n",
      "46/46 [==============================] - 399s 9s/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 1.9661 - val_accuracy: 0.6957\n",
      "Epoch 42/50\n",
      "46/46 [==============================] - 399s 9s/step - loss: 0.0019 - accuracy: 0.9993 - val_loss: 1.9948 - val_accuracy: 0.6957\n",
      "Epoch 43/50\n",
      "46/46 [==============================] - 393s 9s/step - loss: 0.0480 - accuracy: 0.9864 - val_loss: 1.2936 - val_accuracy: 0.6984\n",
      "Epoch 44/50\n",
      "46/46 [==============================] - 396s 9s/step - loss: 0.0323 - accuracy: 0.9905 - val_loss: 1.6028 - val_accuracy: 0.6902\n",
      "Epoch 45/50\n",
      "46/46 [==============================] - 398s 9s/step - loss: 0.0050 - accuracy: 0.9993 - val_loss: 1.7482 - val_accuracy: 0.6929\n",
      "Epoch 46/50\n",
      "46/46 [==============================] - 396s 9s/step - loss: 6.5719e-04 - accuracy: 1.0000 - val_loss: 1.8746 - val_accuracy: 0.6929\n",
      "Epoch 47/50\n",
      "46/46 [==============================] - 398s 9s/step - loss: 3.5285e-04 - accuracy: 1.0000 - val_loss: 1.9290 - val_accuracy: 0.6929\n",
      "Epoch 48/50\n",
      "46/46 [==============================] - 397s 9s/step - loss: 3.7318e-04 - accuracy: 1.0000 - val_loss: 2.0077 - val_accuracy: 0.6984\n",
      "Epoch 49/50\n",
      "46/46 [==============================] - 395s 9s/step - loss: 1.6964e-04 - accuracy: 1.0000 - val_loss: 2.0332 - val_accuracy: 0.7011\n",
      "Epoch 50/50\n",
      "46/46 [==============================] - 395s 9s/step - loss: 1.6016e-04 - accuracy: 1.0000 - val_loss: 2.0604 - val_accuracy: 0.6984\n",
      "6/6 [==============================] - 32s 5s/step - loss: 1.3938 - accuracy: 0.7255\n",
      "Highest Accuracy: 0.7255434989929199 - Loss: 1.3937872648239136\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./1800Model1_9/tokenizer_config.json',\n",
       " './1800Model1_9/special_tokens_map.json',\n",
       " './1800Model1_9/vocab.txt',\n",
       " './1800Model1_9/added_tokens.json',\n",
       " './1800Model1_9/tokenizer.json')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFDistilBertForSequenceClassification, DistilBertTokenizerFast\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Define the model architecture\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "\n",
    "# Get the last transformer layer\n",
    "last_layer = model.distilbert.transformer.layer[-1]\n",
    "\n",
    "# Add a dropout layer\n",
    "last_layer.attention_dropout = tf.keras.layers.Dropout(0.1)\n",
    "last_layer.output_dropout = tf.keras.layers.Dropout(0.1)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=4e-5),\n",
    "              loss=model.compute_loss,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Define the ModelCheckpoint callback to save the best model weights\n",
    "checkpoint_callback = ModelCheckpoint(filepath='best_model_weights.h5',\n",
    "                                      save_best_only=True,\n",
    "                                      save_weights_only=True,\n",
    "                                      monitor='val_accuracy',\n",
    "                                      mode='max')\n",
    "\n",
    "# Train the model with the ModelCheckpoint callback and adjusted parameters\n",
    "model.fit(train_dataset.shuffle(len(train_dataset)).batch(32),\n",
    "          epochs=50,\n",
    "          batch_size=64,\n",
    "          validation_data=test_dataset.shuffle(len(test_dataset)).batch(64),\n",
    "          callbacks=[checkpoint_callback])\n",
    "\n",
    "# Load the weights of the model with the highest validation accuracy\n",
    "model.load_weights('best_model_weights.h5')\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "loss, accuracy = model.evaluate(test_dataset.batch(64))\n",
    "\n",
    "# Print the highest accuracy and loss achieved during training\n",
    "print(f\"Highest Accuracy: {checkpoint_callback.best} - Loss: {loss}\")\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained('./1800Model1_9')\n",
    "tokenizer.save_pretrained('./1800Model1_9')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
